```{r echo=F}
setwd("C:/Users/User/Documents/December2017")
library(knitr)
opts_knit$set(cache=FALSE )
```

Untargeted metabolomics data analysis tutorial
========================================================

#### This tutorial is based on *Feature-oriented mass-spectrometry based metabolomics data analysis LC/MS(qTOF) Data Analysis Using XCMS* by *Maria Vinaixa*

This hands-on tutorial illustrates the approach currently used in our research group to conveniently rank XCMS detected mzRT metabolic features for further MS/MS identification experiments.

Nowadays, feature-oriented mass-spectrometry based metabolomics experiments lead to routinely monitor thousands of mzRT features (i.e., peaks corresponding to individual ions with a unique mass-to-charge ratio (m/z) and a unique retention time (RT)). Each mzRT feature in the dataset is associated with an intensity value (or area under the peak), which indicates its relative abundance in the the biological sample.

In summary the data analysis pipeline for feature-oriented mass-spectrometry based metabolomics is:

1. Raw LC/MS data Organization and Conversion  
  * Scans recorded by the instrument are saved in a vendor data format. Storing various information, for example: ionization mode, retention time, m/z values detected and their corresponding intensity.  
  * Vendor data formats need to be converted to mzXML or mzML  
  * The most common tool is _msconvert_ by _Proteowizard_  
2. XCMS preprocessing  
  * Peak picking (`xcmsSet()`): Analyze the retention time profile of recorded m/z scans and find Gaussian-like peaks time-wise. This functions processes each sample independently and performs peak quantification.   
  * Retention time correction (`retcor()`): Unfortunately, chromatography and mass-spectrometry are not perfectly stable over time. Thus, the retention time values are aligned to make ion peaks comparable between different samples.   
  * Peak grouping (`group()`): Transform ion peaks to _features_. Group ion peaks with a similar retention time and m/z values across samples. This results in a unique value of m/z and retention time (mzRT feature) for different peaks that will be used to compare sample groups.
  * NA value imputation (`fillPeaks()`): If a feature intensity/area value is missing in any sample group, this function imputes an intensity value. After peak grouping, there will always be peak groups that do not include peaks from every sample. This method produces intensity values for those missing samples by integrating raw data in peak group region.
  * If you want to learn more about `xcms`, please check:
    + `xcms` vignette: `vignette("xcmsPreprocess")`
    + [workflow4metabolomics xcms material](http://workflow4metabolomics.org/sites/workflow4metabolomics.org/files/files/w4e-2016-MSprocessing_LC-MS.pdf)
3. Feature matrix data analysis
  * Feature filtering
  * Statistical testing
  * Feature ranking. it is essential to prioritize the list of those detected mzRT features (molecular ions) that will be subsequently submitted to MS/MS experiments to further confirm their identity.This
4. From features to compound identifications
  * Feature annotation: It is important to state that detection of a mzRT feature does not necessary translate into a metabolite entity. Usually, the number of mzRT features is greatly inflated due to the recurrent detection of adducts ([M + Na]+, [M + K]+, etc), isotopes, or doubly charged ions. Several recently launched open-source algorithms, such as CAMERA, and commercially available software such as Mass Hunter (Agilent Technologies) or Sieve (Thermo Scientific), among others are capable of filtering redundancy by annotating isotopes and adduct peaks. 
  This step is not compulsoray and is currently one of the main challenges in untargeted metabolomics data analysis.
  * Database matching: Features that pass statistical significance are considered candidate metabolites that explain the differences found. To identify them, we require using a list of possible adducts and database that represents (as well as possible) the metabolite matrix we should expect in our samples. Though, a database match represents just a putative metabolite identity assignment, we require further experiments to confirm that identification.
  Some examples of metabolites databases:
    + [Human metabolome database](http://hmdb.ca)
    + [Massbank](http://www.massbank.jp/)
    + [Metlin](https://metlin.scripps.edu/)
  * MS/MS experiments: Retention time confirmation and/or MS/MS fragmentation of a model pure compound to that from the ion of interest is necessary to uniquivocally identify metabolites. See [Metabolomics Standard Initiative](http://www.metabolomics-msi.org/).

---------------------------------------------------------

This tutorial assumes that you have a working R installation and the following R packages installed.

```{r results='hide', message=FALSE, warning=FALSE}
library(xcms)
library(ggplot2)
library(reshape)
```

In this tutorial we will use the data that can be found at [Metabolights-MTBLS103](https://www.ebi.ac.uk/metabolights/MTBLS103) LC-MS (Reverse Phase-C18 ESI+ mode) dataset.

Since `xcms` preprocessing is a time consuming process, I provide an *.RData* file that already includes the xcmsSet object.

```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
load("./data/xset3.Rdata")
xset3
```

```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
phenoData(xset3)
```

## Feature filtering

### Sample Representativeness in each mzRT feature

First, we would like to retain those mzRT features wich are consistently found in a majority of samples in
a group. The 80% rule is commonly accepted, meaning that to further consider a mzRT feature it
should be consistently found in 80% of samples of at least one of the experimental groups.

```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
#Determine the number of samples per group
table(xset3@phenoData)
#Calculate the 80% of samples in each experimental group
round(table(xset3@phenoData)*0.8,0)
group.80 <- round(table(xset3@phenoData)*0.80,0)
```

```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
#Obtain the mzRT table
mygroups <- data.frame(xset3@groups)
rownames(mygroups) <- groupnames(xset3)
#Restrict to features found in at least 80% of samples in a group
group.filter <- which(mygroups$CTR>=group.80["CTR"] | mygroups$DISEASE>=group.80["DISEASE"] |
												mygroups$TREATMENT>=group.80["TREATMENT"])
#Calculate percentage
(length(group.filter)/nrow(mygroups))*100 
```

### Intensity

The accuracy of a metabolite identification depends on the quality of fragmentation patterns generated during the MS/MS experiments. MS/MS experiments should focus on parent ions with a minimum intensity threshold to ensure suitable MS/MS fragmentation spectra recording. This threshold depends on the experimental conditions, mainly it depends on the instrument.

```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
#Get intensities from xcmsSet object from those features meeting 80% rule
X1 <- groupval(xset3, value="maxo")
X1 <- X1[group.filter,]
#Get names of mzRT features (M + rounded mz + T + rounded RT)
rownames(X1) <- groupnames(xset3)[group.filter]
X1[1:5,1:10]
```

```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
#Define experimental groups
myclass <- as.factor(xset3@phenoData$class)
table(myclass)
```

This plot shows the intensity profiles of each sample group, as we can see they are comparable between all groups. 

```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
#Compute the mean intensities for each group
meanintensities<-t(apply(X1, 1, function(x) tapply(x, myclass, mean)))
rownames(meanintensities) <- rownames(X1)

## Plot Intensity distributions /density plots

# For the first plot we need to sort mean intensities in each group 
M2 <- apply(meanintensities,2,sort,decreasing=TRUE)
M2plot <- melt(M2)
names(M2plot) <- c("Features","group","Intensity")

#Plot intensities profile
ggplot(data=M2plot,aes(x=Features, y=Intensity, colour=group))+ 
  geom_line()+
  scale_y_log10()+
  labs(y = "Intensity", x = "# mzRT Features", title= "Intensity")+ 
	annotation_logticks()
```

If we take a look at the plot of the distribution of intensity, we can observe that there is a large number of ions that have about XXX units/counts of intensity, this we could call it the "background", which includes noise and low intensity peaks, we are not interested in these peaks.
```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
ggplot(data=M2plot,aes(x=Features, y=Intensity, colour=group))+ 
  geom_line()+
	geom_hline(yintercept = 5000, linetype=2)+  
	scale_y_log10()+
  labs(y = "Intensity", x = "# mzRT Features", title= "Intensity")+ 
	annotation_logticks()

```


Based on our experience with the similar experiments and seeing that most of the peaks have large intensity we decide to set the minimum intensity at 5000.

```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
#Stablish intensity threshold value
thresholdvalue <- 5000
#Getting number of features with mean intensity above certain 
#threshold counts in at least one of the groups except QC group
intensity.filter <- meanintensities[,which(colnames(meanintensities)!="QC")]
idx_i <- rownames(meanintensities[apply(intensity.filter, 1, function(x) any(x>thresholdvalue)==TRUE),])
#Getting the percentage out of the total mzRT features meeting the intensity threshold
(length(idx_i)/nrow(meanintensities))*100 
```

### Handling analytical variation through Quality Control Samples (QCs)

The most common source of variation in LC-MS experiments are due to sample preparation (human variation) and instrumental deviation during the chromatography run (analytical variation).

The ideal method to examine analytical variation is to analyze quality control (QC) samples, which will provide robust quality assurance of each detected mzRT feature. To this end, QC samples should be prepared by pooling aliquots of each individual sample entering the study and analyze them periodically throughout the sample work list. Being replicates of the same pooled samples, QC samples are expected to not contain biological variation.

A powerful tool to examine sources of variation in a dataset is principal components analysis (PCA). PCA is a very popular unsupervised multivariate technique for exploration and reduction of high-dimensional data. PCA basically summarizes the variation in the original dataset to a set of uncorrelated components, each of which is a particular linear combination of the original variables. It is always advisable to perform a preliminary exploratory PCA analysis on a the dataset including both, biological samples and QC samples. This will reveal unestimable information of main patterns and trends within the dataset. The performance of the analytical platform can be calculated individually for each detected mzRT feature computing the variation of each mzRT feature around their mean in QC pooled samples.


```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
## Compute and plot PCA for QCs check and to explore main trends in the data

## 1-. Create a dataframe containing intensity data
  D1 <- data.frame(X1)
## 2-. Restrict this dataframe to intensities above the fixed threshold value
  D1 <- D1[idx_i,]
## 3-. Row-wise normalzation prior to PCA: Normalize across samples each mzRT variable to max=1
  D1norm <- data.frame(t(apply(D1,2, function(x) (x/max(x)))))
## 4-. Compute PCA
  pca <- prcomp(D1norm)
## 5-. Print variance summary for the fourth first PCs
  summary(pca)$importance[, 1:4]
## 6-. Get scores values
  scores <- data.frame(pca$x[,c("PC1","PC2")])
  scores$class <- myclass
## 7-. Labelling samples
  lab <- as.numeric(gsub("\\D","",colnames(D1)))
  #7-. Plot scores
scores.plot <- ggplot(data=scores,aes(x=PC1, y=PC2, colour=class))+
  geom_point(alpha = I(0.7), size=10)+
  geom_text(data=scores, mapping=aes(x=PC1, y=PC2, label=lab),size=4, vjust=3, hjust=3)+
  geom_hline(yintercept = 0)+
  geom_vline(xintercept = 0)+
	stat_ellipse()

scores.plot
```

Examine analytical variation in the dataset using PCA. Determine the percentage of features having more analytical variation than other sources of variation. 

Explore the xcmsSet object using PCA analysis. Represent PCA scores plot and try to interpret it. Do you think that the clustering trend of QC samples is as it should?. Is there any rationale? According to the injection sequence of QC samples, was there any analytical drift during sample acquisition?

### Diagnosing QC Samples

What if we observe an unexpected QC behaviour? QCs are our source of information in terms of instrument variability along the LC-MS run. In case we detect that QCs positioning in the PCA seems to depend on the injection order, we can use the following plots to check if the data itself can reveal us what or when it happened.

*QC intensity boxplots.* Boxplots provide a general view of the distribution of values in our QC samples, independently of feature. We expect them to be near exact to each other.

```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
M3  <- X1[,which(myclass=="QC")]
M3plot <- data.frame(melt(M3))
colnames(M3plot) <- c("Features","QC_Sample","Intensity")

p3 <- ggplot(M3plot,aes(x=QC_Sample,y=Intensity))+
	geom_boxplot()+
	scale_y_log10()

p3
```

*QC total intensity.* QC total sum of ions intensity, in other words, the total of signal recorded should be comparable between QCs. If we observe that there is a large deviation at some point, or that there is a linear or non-linear tendency to gain/lose intensity, we may have to use a method for signal correction. Signal correction is a delicate approach, and a few different methods exist (LOESS, LOWESS, RSLC), they perform mathematical approximations with the intention to make the QC signal profile as uniform as possible, and then interpolate these calculations to the sample values as well.
```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
totalI <- apply(M3,2,sum)
M3plot2 <- data.frame("QC_Sample"=names(totalI),"TotalIntensity"=totalI)
M3plot2$TotalIntensity <- 
	(M3plot2$TotalIntensity-mean(M3plot2$TotalIntensity))*100/mean(M3plot2$TotalIntensity) 
p4 <- ggplot(M3plot2,aes(x=QC_Sample,y=TotalIntensity,group=1))+
	geom_point()+
	geom_line()+
	ylab("% Relative Deviation")

p4
```

*QC retention time shifts.* Given that chromatography systems are becoming more and more stable, the fact that retention time would vary during the LC-MS run is a rare phenomenon. In the chance that it occurred, the coefficient of variation (CV) at a given retention time window would be notably larger than the rest of the run.
```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
# Define (CV) function
co.var <- function(x) ( 100*sd(x)/mean(x) )
qc.var <- apply(M3,1,  co.var)
qc.RT <- mygroups[names(qc.var),"rtmed"]
M5plot <- data.frame("RT"=qc.RT,"IntensityCV"=qc.var)

p5 <- ggplot(M5plot,aes(x=RT,y=IntensityCV))+
	geom_line()
p5
```

Once assured that there is not a major reason for signal correction or repeating our experiment, it is time to continue with feature filtering and statistical analysis.


### Compute CV across samples and QCs

The performance of the analytical platform can be calculated individually for each detected mzRT feature computing the variation of each mzRT feature around their mean in QC pooled samples (CVQC). Likewise, we can also calculate the variation of mzRT features around their mean in the samples entering the experiment (CVS). This variation would enclose both, analytical and biological variation.

Consequently, those mzRT features where CVQC > CVS contain more analytical variation than other sources of variation and they should be conveniently removed from further analysis. This criteria helps to focus on mzRT features with the lowest proportion of analytical variation.

Finally combine intensity and QC criteria to reduce the initially detected number of mzRT features.

```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
# Define  QC and Sample samples classes
cl1 <- rep(c("Sample","QC"), times=c(38,8))
# Compute CV for QC and Samples
CV<-t(apply(X1, 1, function(x) tapply(x, cl1, co.var)));
# Determine the percentage of features where CV(Samples)>CV(QC)
idx_qc <- rownames(CV)[CV[,"Sample"]>CV[,"QC"]]
length(idx_qc)/nrow(X1)*100
```

```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
cat("Number of features that pass Intensity filter:",length(idx_i))
cat("Number of features that pass CV filter:",length(idx_qc))
```

```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
#Combine both intensity and QC criteria
Ib <- intersect(idx_i,idx_qc)
#Calculate the number of retained mzRT features
cat("Number of features that pass both filters:",length(Ib))
length(Ib)/nrow(X1)*100
```


## Statistical Analysis

```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
#Create a new dataset without QC samples and with those mzRT features
X1 <- t(data.frame(X1))
D2 <- subset(X1, myclass!="QC", select=Ib)
D2 <- data.frame(t(D2))
D2[1:4,1:5]
```

Provided the limitation in the number of MS/MS confirmation experiments that can be run, additional criteria to those described above are necessary to further reduce the number of initial mzRT features to an amenable number. 

In our pipeline once QC and intensity criteria has been applied, the resulting dataset is usually interrogated via hypothesis testing to select mzRT features showing statistical significance. It is important to note that datasets derived from feature-oriented mass-spectrometry based metabolomics are both high-dimensional and multicollinear. We are presented with datasets containing thousands of detected mzRT with a rather limited number of individuals or samples per group. Additionally, these features present multiple correlations since a metabolite is usually represented by more than one mzRT feature resulting from isotopic distributions, potential adducts, and in-source fragmentation. Moreover, the evident biochemical interrelation among metabolites may also contribute to the redundancy. 

In untargeted LC-MS-based metabolomics studies, the number mzRT features tested in a univariate manner ranges in the thousands. As the number of hypotheses tests increases, so does the probability of wrongly rejecting a null hypothesis because of random chance and therefore a substantial number of false positives (Type I error) may occur. This accumulation of false positives is termed the *multiple testing problem* and is a general property of a confidence-based statistical test when applied across multiple variables. 

From a metabolomics research standpoint, Type I errors are particularly undesirable. A substantial amount of work and resources are required to setup MS/MS confirmation experiments. In the worst case, a follow-up validation study on a false positive finding would not replicate the original work with consequent waste of resources and time. In such situations the chance for false positive rates must be carefully handed and therefore, it is necessary to correct for multiple testing. 

```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
#Get rid out of QCs in factor defining experimental groups
myclass <- as.factor(xset3@phenoData$class[xset3@phenoData$class!="QC"])
#Perform an anova comparison of groups (CTR, DISEASE & TREATMENT)
pm <- matrix(ncol=length(levels(myclass)),nrow=nrow(D2))
for(i in 1:nrow(D2)){
	tempdata <- data.frame("value"=as.numeric(D2[i,]),"class"=myclass)
  aov.out <- aov(value ~ class, data=tempdata)
  multcomp <- TukeyHSD(aov.out)
  pm[i,]  <- as.matrix(multcomp[["class"]][,"p adj"])
}
rownames(pm) <- rownames(D2)
colnames(pm) <- rownames(multcomp[["class"]])
#Adjust for multiple testing using false discovery rate
p.val.adj.DISEASE.CTR <- p.adjust(pm[,"DISEASE-CTR"],"fdr");
p.val.adj.TREATMENT.DISEASE <- p.adjust(pm[,"TREATMENT-DISEASE"],"fdr");
```

In addition to statistical significance, a common practice to identify mzRT features of relevance within a dataset is to rank these features according their fold-change (FC). FC is the ratio of intensity between two conditions, normally taking the control case as a reference, and it can be thought as the magnitude of change calculated between two populations under study.

```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
#Create the fold change function
fc.test<-function(df,classvec,classA,classB){
	coln <- names(df)
  case <- df[which(coln==classA)];control <- df[which(coln==classB)]
  logFC <- log2(case/control)
  FC <- case/control;
  FC2 <- -control/case
  FC[FC<1] <- FC2[FC<1]
  fc.res <- c(FC,logFC)
	names(fc.res) <- c("FC","logFC")
  return(fc.res)
}
```

```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
#Calculate mean intensities 
meanintensities  <- t(apply(D2,1,tapply,myclass,mean))

#Calculate FC for "DISEASE-CTR" groups 
fc.res <- t(apply(meanintensities,1, function(x) fc.test(x,myclass,"DISEASE","CTR")))
hist(fc.res[,"logFC"],breaks=100)
```

We may be more or less restrictive depending on the number of significant features found. In our case, we set the FC threshold to *2*. 

```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
#Calculate FC for "DISEASE-CONTROL" groups
R1 <- data.frame(fc.res,p.val.adj.DISEASE.CTR)
I.DISEASE.TREATMENT <- (abs(R1$FC) > 2 & R1$p.val.adj.DISEASE.CTR< 0.05)
R1$threshold <- as.factor(I.DISEASE.TREATMENT)
table(R1$threshold)
```

```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
#Calculate FC for "DISEASE-TREATMENT" groups 
fc.res2 <- t(apply(meanintensities,1, function(x) fc.test(x,myclass,"TREATMENT","DISEASE")))
R2 <- data.frame(fc.res2,p.val.adj.TREATMENT.DISEASE)
I.DISEASE.CTR <- (abs(R2$FC) > 2 & R2$p.val.adj.TREATMENT.DISEASE< 0.05) 
R2$threshold <- as.factor(I.DISEASE.CTR)
table(R2$threshold)
```


Examine difeerences derived from CTR, TREATMENT and DISEASE states in the example dataset using Volcano Plot. Which comparison shows the highest diferences?
```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
#Draw Volcano plots for both comparisons
p1 <- ggplot(data=R1, aes(x=R1$logFC, y=-log10(R1$p.val.adj.DISEASE.CTR),
                          colour=threshold))+
  geom_point(alpha=0.4, size=1.75) +
  theme(legend.position="none") +
  xlim(-10, 10)+
  geom_hline(yintercept = -log10(0.05))+
  geom_vline(xintercept = log2(2))+
  geom_vline(xintercept = -log2(2))+
  labs(x = "log2(FC)", y = "-log10(p.adj)", title = "DISEASE vs CTR")
p2 <- ggplot(data=R2, aes(x=R2$logFC, y=-log10(R2$p.val.adj.TREATMENT.DISEASE), 
                          colour=threshold))+
  geom_point(alpha=0.4, size=1.75) +
  theme(legend.position="none") +
  xlim(-10, 10)+
  geom_hline(yintercept = -log10(0.05))+
  geom_vline(xintercept = log2(2))+
  geom_vline(xintercept = -log2(2))+
  labs(x = "log2(FC)", y = "-log10(p.adj)", title = "DISEASE vs TREATMENT")

p1
p2
```

## Collapse Relevant Information to subsequent MS/MS experiments

The final step in an untargeted metabolomics experiment is the identification of the features of interest, features that may explain the differences between our sample groups. This list of features are then searched in a metabolite database looking for matches. 

There are two main concepts to take into account when performing this search:
* Adducts: During ionisation in the mass-spectrometer, compound ions and solvent ions (such as K+ or Na+) may become bound due to ionic interactions, meaning that the m/z of the feature is different of the monoisotopic mass of the non-ionised compound. Thus, a list of adducts and their corresponding mass shifts are required to interrogate the database.
* Mass accuracy error: Due to electronic limitations mass-spectrometers cannot measure the m/z perfectly, meaning that there is an amount of inexactitude in the m/z values in the features. This mainly depends on the type of mass-spectrometer and technical aspects (calibration and state of the instrument). In consequence, a PPM has to be considered when searching for matches in the database.

Collect all necessary information to subsequent MS/MS experiments and summarize it in a ".csv" file.

```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
#Combine FC values as a criteria to sort features
idx.s <- union(which(I.DISEASE.TREATMENT),
							 which(I.DISEASE.CTR))
# This step is useful to rank fold-changes if there is more than 1 sample group
FC.combi <- abs(R1$FC*R2$FC)
Rf <- data.frame(R1,R2,FC.combi)
Rf.s <- Rf[idx.s,]
# sort fold change values
Rf.s.ord <- Rf.s[order(Rf.s$FC.combi, decreasing = TRUE), ]
sig.ord <- rownames(Rf.s.ord)
#Retrieving mzRT information
final <- mygroups[sig.ord,c("mzmed","rtmed")]
#Getting mean group intensities for significative features
intord <- meanintensities[sig.ord,]
#Collapse all information
feature.msms <- data.frame(final,Rf.s.ord,intord)
dim(feature.msms)
head(feature.msms)
#Write it in a .csv file
write.csv(file="resultstoMSMS.csv", x=feature.msms)
```

There is two options when performing your putative identification search against metabolite databases:
* Perform batch searches in METLIN -link- or HMDB -link-. Where you have to load a list of m/z values and select from a list of adducts and define a PPM error.
+ The database is always up-to-date
- "Limited" number of adducts
* Download (or build your own) a database and perform the search programmatically.
+ Flexibility
+ Any adducts can be search (in-house list of adducts)
- Needs to be updated

How to download HMDB database:
Go to hmdb.ca
Browse Metabolites > Extract data > Apply filters if necessary > Select columns (Name, monoisotopic mass) > Download


negative <- read.table("Negative_ESI.txt",header=T,stringsAsFactors=F)
positive <- read.table("Positive_ESI.txt",header=T,stringsAsFactors=F) 
hmdb <- read.table("C:/Users/User/Documents/R-CWD/HMDB_17_09.txt",header=T,sep="\t",
									 stringsAsFactors=F,fill=T,quote="")

aixo es l'script

input és el vector de masses en forma numerica, ara mateix esta per fer negatiu si ho vols canviar fes un reemplazar "negative" per "positive".

Si no ten ensurts dema ho mirem



input <- itworksdf$mzmed
ppm <- 10

hmdbhits <- lapply(input,function(x){
	search_vect <- (x+negative$IsotopicMass)
	h <- lapply(search_vect,function(x){
	mass_range <- c(x-ppm*(x/1e6),x+ppm*(x/1e6))
	a <- which(hmdb$MonoisotopicMass>mass_range[1] & hmdb$MonoisotopicMass<mass_range[2]) 
	})
	
	h2 <- sapply(1:length(h),function(x){
		y <- h[[x]]	
		if(length(y)>0){
				r <- paste(hmdb$Name[y],sep="",collapse=";")
		}else{
			r <- "No hit"
		}
		return(r)
	})
	
	return(h2)
})

*****
*****

Untargeted isotope labelling analysis with geoRge
========================================================

# Labelling experiments in metabolomics

The use of stable isotope compounds in metabolism experiments has been an strategy to enlight the way enzymes perform their role and how metabolites are transformed. In general, the sources of labelling are metabolites that are essential for cell metabolism, such glucose, glutamine or glycerol. This labeled metabolites are used in cell culturing instead of their non-labeled homologous for a limited amount of hours. The labelled metabolites then are incorporated and metabolized as needed, distributing their labelled atoms through the metabolic pathways.

Isotopic labelling in metabolomics allows the interrogation of the metabolism fluxes, the amount of metabolic reactions performed per time, this is accomplished by measuring the amount of labeled derivates of a compopund (isotopologues), either in absolute concentrations (targeted methods) or in a relative manner (untargeted). Isotopologues, are compounds with the same chemical formula, i.e. identity, but different mass due to divergent atom composition.

## geoRge, a tool for isotope labelling untargeted metabolomics experiments.

As seen above, the large number of features obtained during an untargeted metabolomics experiment has to be addressed by making assumptions and filtering to reduce our number of features in our dataset. 

The main objective in this type of approaches is to determine which clusters of features belong to labelled isotopologues of the same metabolite, the number of atoms of their structure are labelled and the amount of labelling they experienced.

There is a variety of tools that can be used for isotope labelling untargeted metabolomics data analysis, each using its own algorithm and thus different advantages and disadvantages, in terms of speed, sensibility, sensitivity and number of samples necessary to run.

In the case of geoRge, we assume that labelled isotopologues are equivalent to "significant features" when comparing non-labelled and labeled samples, thus they are detectable using a similar workflow as we saw in the former part of this document, by performing statistical and fold-change testing. In consequence, the main drawback of this approach is that the detection of labelled features depends on the statistical power of the experimental design, i.e. number of replicates and replicate CV.

## geoRge pipeline

geoRge is currently only available in GitHub (github.com/jcapelladesto/george), so in this case we need to install the package using **devtools**.
```{r eval=FALSE}
#install.packages("devtools", dependencies=TRUE)
library(devtools) 

install_github("jcapelladesto/geoRge")

```

The package itself contains an example **xcmsSet** from Metabolights 213 (link).

```{r echo=FALSE}
library(geoRge) 
data(mtbls213)
```

In our case the dataset contains a collection of non-labelled samples and labelled samples were cultured in glucose with its 6 carbons labelled with its C13 isotope which weights 13.003 instead of 12.000 and there is two sample classes, normoglycemic and hyperglycemic (normal and high glucose concentration), 3 replicates each.

```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
mtbls213@phenoData
```

The first function, **PuInc_seeker** (Putative incoporation) performs a comparison between labelled and non-labelled sample groups. The user can provide as many groups as desired, as long as, there is both labelled and non-labelled samples.

```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
s1 <- PuInc_seeker(XCMSet=mtbls213,ULtag="CELL_Glc12",Ltag="CELL_Glc13",
sep.pos.front=TRUE ,fc.threshold=1.5,p.value.threshold=.05)
```

The second function, **basepeak_finder** will try to match each labelled feature with its corresponding monoisotopic (not labelled) m/z, thus giving the m/z that can be matched to the metabolite entity. 

```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
s2 <- basepeak_finder(PuIncR = s1, XCMSet = mtbls213, UL.atomM=12.0,L.atomM=13.003355,
	ppm.s=6.5,Basepeak.minInt=2000)
```

If there is more than one group in the experiment, we can compare the labelling between 2 groups using **label_compare()** function:

```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
s3 <- label_compare(geoRgeR = s2, XCMSet =mtbls213, rt.win.min = 1, 
										control.cond = "05mM_Normo",ppm.s=6.5,
										fc.vs.Control = 1, p.value.vs.Control = 0.05, Show.bp = T)
```


The package also includes an internal database and a list of most common adducts. The function **database_query()** can be used to directly match the monoisotopic peak of labelled features.

```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
negative <- read.table(system.file("extdata/adducts_negative.txt", package="geoRge"),
                       header=T,stringsAsFactors=F)

db <- read.csv(system.file("extdata/ExampleDatabase.csv", package="geoRge"),
                header=T,stringsAsFactors=F,fill=T)

hits <- database_query(geoRgeR = s2, adducts = negative, db = db)

head(hits)
```


```{r fig.align="center", fig.width=10, tidy=TRUE, comment=NA}
final <- cbind(s2$geoRge,s3,hits)
head(final)
```


As in standard untargeted metabolomics experiments, the features identity needs to be confirmed through MS/MS experiments and/or retention time confirmation. An additional level of information that can be extracted is that, if labelled isotopologues are fragmented, it is possible to elucidate which part of the molecule contained the **x** number of labelled atoms.

****
****

## BONUS: Other useful functions for metabolomics data analysis
### t-test test code
```{r}
my.t.test<-function(df,classvec,classA,classB,var.equal=T){
  	coln <- names(df)
    case <- df[which(coln==classA)];control <- df[which(coln==classB)]
    t.res <- t.test(case,control,var.equal=var.equal)$p.value
    return(t.res)
}

```

